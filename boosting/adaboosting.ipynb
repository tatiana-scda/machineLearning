{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementação do Algoritmo de Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting \n",
    "\n",
    "_Boosting_ é um processo que utiliza um conjunto de modelos de solução fraca (_weak learner_) para atingir melhor acurácia e criar uma solução forte (_strong learner_).\n",
    "\n",
    "Os _weak learners_ no Adaboost são árvores de decisão de primeiro nível, chamados de _stumps_. Na primeira iteração, todos os pesos do algoritmo iniciam iguais e após isso, as decisão classificadas incorretamente passam a ter maior peso do que as classificadas corretamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente importamos as bibliotecas necessárias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em sequência fazemos a leitura e estruturação dos dados de entrada.\n",
    "\n",
    "Para isso vamos separar nossos dados em dois grupos, `features` e `labels`. \n",
    "\n",
    "| Grupo    | Descrição                  |\n",
    "| :------- | :------------------------- |\n",
    "| features | caractetirsticas dos dados |\n",
    "| labels   | classificação              |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open pandas dataframe\n",
    "data = pd.read_csv('tic-tac-toe.data', header=None).sample(frac=1)\n",
    "\n",
    "# List of labels and features\n",
    "original_features = []\n",
    "original_labels = []\n",
    "\n",
    "# Labels\n",
    "original_labels = data[data.columns[9]].values \n",
    "del data[data.columns[9]] \n",
    "original_labels = pd.Categorical(original_labels).codes\n",
    "original_labels = [1 if i == 1 else -1 for i in original_labels]\n",
    "original_labels = np.asarray(original_labels)\n",
    "\n",
    "# Features\n",
    "original_features = data.values\n",
    "original_features = np.asarray(original_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após essa separação podemos imprimir as classes na variavel `labels` para verificar a transformação em dado categórico. \n",
    "A escolha para manipulação como valor negativo e positivo se dá por tornar mais intuitivo os valores corrependentes como positivos e negativos, além de possibilitar a troca de valor das classificações incorretas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1, -1,  1, -1,  1, -1, -1,  1,  1,  1,  1,  1,  1,  1,\n",
       "       -1, -1,  1,  1, -1,  1,  1,  1,  1, -1,  1, -1, -1,  1,  1, -1,  1,\n",
       "        1, -1,  1, -1,  1,  1, -1,  1,  1, -1,  1, -1, -1,  1,  1,  1,  1,\n",
       "        1,  1,  1, -1,  1,  1,  1,  1,  1,  1, -1, -1, -1,  1,  1,  1,  1,\n",
       "       -1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,\n",
       "       -1,  1, -1,  1,  1, -1,  1, -1,  1,  1, -1,  1,  1,  1,  1,  1,  1,\n",
       "       -1, -1, -1,  1,  1,  1,  1, -1,  1,  1,  1, -1, -1,  1,  1,  1,  1,\n",
       "       -1,  1,  1,  1, -1,  1,  1, -1,  1, -1,  1, -1,  1,  1,  1,  1,  1,\n",
       "       -1,  1, -1, -1,  1,  1,  1, -1, -1,  1, -1,  1, -1,  1,  1, -1, -1,\n",
       "        1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1, -1, -1,  1, -1,  1,\n",
       "        1,  1,  1,  1,  1, -1, -1,  1,  1, -1,  1,  1, -1,  1,  1, -1, -1,\n",
       "        1, -1, -1,  1,  1,  1,  1, -1,  1, -1,  1,  1,  1,  1,  1, -1,  1,\n",
       "       -1, -1,  1,  1,  1,  1, -1,  1, -1,  1,  1,  1,  1,  1, -1, -1,  1,\n",
       "       -1,  1, -1,  1, -1,  1,  1,  1,  1, -1,  1, -1,  1,  1, -1, -1,  1,\n",
       "       -1,  1, -1, -1, -1, -1,  1, -1,  1,  1,  1,  1,  1,  1, -1,  1, -1,\n",
       "        1,  1,  1,  1,  1, -1, -1, -1, -1, -1,  1,  1, -1, -1,  1, -1,  1,\n",
       "        1,  1,  1, -1,  1,  1,  1,  1, -1, -1, -1,  1,  1,  1, -1,  1,  1,\n",
       "        1,  1,  1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1,  1, -1,  1, -1,\n",
       "        1,  1, -1,  1,  1, -1,  1,  1,  1,  1, -1,  1, -1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1, -1, -1,  1,  1,  1,\n",
       "        1,  1,  1, -1,  1,  1,  1,  1, -1,  1,  1,  1, -1,  1,  1,  1,  1,\n",
       "        1,  1, -1,  1,  1,  1, -1,  1,  1, -1, -1, -1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1, -1,\n",
       "        1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1, -1, -1, -1,  1,  1,\n",
       "        1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1, -1, -1,  1,  1, -1,  1,\n",
       "        1,  1,  1,  1, -1, -1, -1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "       -1, -1,  1,  1, -1, -1,  1,  1,  1, -1, -1,  1, -1, -1,  1,  1,  1,\n",
       "        1, -1,  1,  1, -1, -1,  1,  1, -1,  1, -1,  1, -1,  1,  1, -1,  1,\n",
       "       -1,  1,  1,  1, -1,  1, -1,  1,  1,  1, -1, -1, -1, -1, -1,  1,  1,\n",
       "        1,  1, -1,  1, -1,  1,  1, -1,  1,  1, -1, -1,  1,  1,  1,  1,  1,\n",
       "        1,  1, -1,  1, -1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1, -1,  1, -1, -1,  1, -1,  1,  1,  1, -1,  1,  1, -1, -1,\n",
       "       -1,  1, -1,  1,  1,  1,  1,  1,  1, -1,  1, -1,  1, -1,  1,  1, -1,\n",
       "        1,  1,  1, -1,  1,  1,  1, -1, -1, -1,  1,  1, -1,  1,  1,  1, -1,\n",
       "        1,  1, -1, -1, -1, -1,  1, -1, -1,  1, -1,  1, -1,  1, -1, -1, -1,\n",
       "       -1, -1, -1,  1,  1, -1, -1, -1, -1,  1,  1,  1, -1,  1,  1,  1, -1,\n",
       "       -1, -1, -1,  1,  1,  1,  1,  1,  1, -1, -1, -1,  1,  1,  1,  1,  1,\n",
       "        1, -1,  1, -1, -1,  1,  1,  1, -1, -1,  1, -1,  1, -1, -1,  1,  1,\n",
       "        1, -1,  1, -1, -1,  1,  1, -1, -1,  1,  1, -1, -1, -1, -1,  1,  1,\n",
       "       -1,  1,  1,  1, -1,  1, -1,  1,  1, -1, -1,  1, -1, -1,  1,  1,  1,\n",
       "       -1,  1, -1, -1,  1,  1,  1,  1,  1, -1,  1, -1, -1,  1,  1,  1, -1,\n",
       "        1,  1,  1, -1, -1, -1,  1, -1,  1,  1, -1, -1,  1,  1, -1,  1, -1,\n",
       "        1,  1, -1, -1,  1, -1,  1,  1,  1, -1,  1,  1,  1,  1, -1,  1,  1,\n",
       "        1,  1, -1, -1,  1,  1, -1, -1,  1,  1,  1,  1, -1,  1,  1, -1,  1,\n",
       "       -1,  1,  1,  1, -1, -1,  1,  1,  1, -1,  1, -1,  1,  1,  1,  1, -1,\n",
       "        1,  1,  1,  1, -1,  1,  1, -1, -1, -1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1, -1,  1, -1,  1, -1,  1, -1, -1,  1,  1,  1,  1,\n",
       "       -1, -1,  1, -1,  1,  1, -1,  1, -1, -1, -1,  1, -1, -1,  1,  1,  1,\n",
       "        1,  1, -1, -1,  1,  1, -1, -1,  1,  1,  1,  1,  1, -1,  1, -1,  1,\n",
       "       -1, -1, -1,  1,  1,  1, -1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1, -1, -1, -1,  1,  1, -1, -1,  1,  1,  1, -1,  1, -1,  1,  1, -1,\n",
       "        1, -1,  1, -1,  1,  1,  1, -1,  1,  1,  1,  1,  1, -1, -1,  1,  1,\n",
       "        1, -1, -1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1, -1, -1,\n",
       "        1, -1,  1, -1,  1,  1,  1,  1, -1,  1,  1,  1, -1,  1,  1, -1,  1,\n",
       "        1,  1,  1, -1,  1, -1, -1,  1,  1,  1,  1,  1, -1, -1, -1,  1,  1,\n",
       "        1, -1,  1,  1,  1,  1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(original_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Classificador - AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez que temos os dados discriminados, agora precisamos do algoritmo de classificação. O Adaboost irá iniciar os atributos da sua classe, calcular predição e erro de cada stump e compará-los para escolher o melhor modelo. Então é feita a comparação e cálculo de acurácia e erro. As escolhas de modelos, acurárias e erros serão aplicadas no boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    \"\"\"\n",
    "    Adaboost Classifier\n",
    "    \"\"\"\n",
    "\n",
    "    FEATURES = 'features'\n",
    "    ATTRIBUTE = 'attribute'\n",
    "    CLASS = 'class'\n",
    "    VALUE = 'value'\n",
    "    STATE = 'state'\n",
    "    ERROR = 'error'\n",
    "    PREDICTIONS = 'predictions'\n",
    "    POSSIBLE_VALUES = ['x', 'o', 'b']\n",
    "    POSSIBLE_STATES = [1, -1]\n",
    "\n",
    "    def __init__(self, training_set, testing_set):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        \"\"\"\n",
    "\n",
    "        # Class variables\n",
    "        self.training_set = training_set\n",
    "        self.testing_set = testing_set\n",
    "        self.features_len = training_set[self.FEATURES].shape[1]\n",
    "        self.train_len = training_set[self.FEATURES].shape[0]\n",
    "        self.test_len = testing_set[self.FEATURES].shape[0]\n",
    "        self.weights = np.divide(np.ones(self.train_len), self.train_len)\n",
    "        self.ensemble = []\n",
    "        self.alpha = []\n",
    "\n",
    "    def evaluate_stump(self, stump):\n",
    "        \"\"\"\n",
    "        Evaluate stump individualy\n",
    "        \"\"\"\n",
    "        \n",
    "        # Stump evaluate parameters\n",
    "        predictions = np.zeros(self.train_len)\n",
    "        pred_errors = np.ones(self.train_len)\n",
    "\n",
    "        a = stump[self.ATTRIBUTE]\n",
    "\n",
    "        # Evaluate stump based on training data\n",
    "        for i in range(self.train_len):\n",
    "            value = self.training_set[self.FEATURES][i][a]\n",
    "            output = self.training_set[self.CLASS][i]\n",
    "\n",
    "            if value == stump[self.VALUE]:\n",
    "                predictions[i] = stump[self.STATE]\n",
    "            else:\n",
    "                predictions[i] = stump[self.STATE] * -1\n",
    "\n",
    "            if predictions[i] == output:\n",
    "                pred_errors[i] = 0\n",
    "\n",
    "        # Stump error\n",
    "        error = np.sum(np.multiply(self.weights, pred_errors))\n",
    "        \n",
    "        return error, predictions\n",
    "\n",
    "    def find_best_stump(self):\n",
    "        \"\"\"\n",
    "        Compare stumps and find stump with lowest error\n",
    "        \"\"\"\n",
    "        \n",
    "        # Inf - higher error for stump compare\n",
    "        lowest_error = float('inf')\n",
    "\n",
    "        # Possible values\n",
    "        possible_values = self.POSSIBLE_VALUES\n",
    "        possible_states = self.POSSIBLE_STATES\n",
    "\n",
    "        # Find stump with lowest error\n",
    "        for a in range(self.features_len):\n",
    "            for value in possible_values:\n",
    "                for state in possible_states:\n",
    "                    stump = {self.ATTRIBUTE: a, self.VALUE: value, self.STATE: state}\n",
    "\n",
    "                    error, predictions = self.evaluate_stump(stump)\n",
    "                    \n",
    "                    stump[self.ERROR] = error\n",
    "                    stump[self.PREDICTIONS] = predictions\n",
    "                    \n",
    "                    if error < lowest_error:\n",
    "                        lowest_error = error\n",
    "                        best_stump = stump\n",
    "\n",
    "        return best_stump\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Calc accuracy and arror\n",
    "        \"\"\"\n",
    "        \n",
    "        correct = 0\n",
    "\n",
    "        # Prediction based on test data\n",
    "        for i in range(self.test_len):\n",
    "            H = 0  \n",
    "\n",
    "            for model in range(len(self.ensemble)):\n",
    "                a = self.ensemble[model][self.ATTRIBUTE]\n",
    "                value = self.testing_set[self.FEATURES][i][a]\n",
    "\n",
    "                if value == self.ensemble[model][self.VALUE]:\n",
    "                    prediction = self.ensemble[model][self.STATE]\n",
    "                else:\n",
    "                    prediction = self.ensemble[model][self.STATE] * -1\n",
    "\n",
    "                H += self.alpha[model] * prediction\n",
    "\n",
    "            H = np.sign(H)\n",
    "\n",
    "            if H == self.testing_set[self.CLASS][i]:\n",
    "                correct += 1\n",
    "\n",
    "        # Accuracy and error\n",
    "        accuracy = (correct / self.test_len) * 100\n",
    "        error = 100 - accuracy\n",
    "\n",
    "        return accuracy, error\n",
    "\n",
    "    def boost(self, num_stumps):\n",
    "        \"\"\"\n",
    "        Redefine alpha weights\n",
    "        \"\"\"\n",
    "        \n",
    "        models = []\n",
    "        \n",
    "        # Redefine weights for each stump\n",
    "        for i in range(num_stumps):\n",
    "            best_model = self.find_best_stump()\n",
    "            \n",
    "            alpha = 0.5 * np.log((1 - best_model[self.ERROR]) / best_model[self.ERROR])\n",
    "            self.alpha.append(alpha) \n",
    "            self.ensemble.append(best_model)\n",
    "            \n",
    "            results = self.evaluate()          \n",
    "            accuracy = results[0]\n",
    "            error = results[1]\n",
    "            models.append(best_model)\n",
    "\n",
    "            self.weights = np.multiply(self.weights,np.exp(-1 * self.alpha[i] * np.multiply(self.training_set[self.CLASS], best_model[self.PREDICTIONS])))\n",
    "            self.weights = np.divide(self.weights, np.sum(self.weights))\n",
    "\n",
    "        return accuracy, error, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rodando o algoritmo com K Folds, estamos definindo que os dados serão utilizados todos (_k_) vezes e em cad auma destas vezes, um diferente set de dados será treino e teste. Esta implementação evita o overfitting e torna o algoritmo mais genérico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(num_boosts):\n",
    "    FEATURES = 'features'\n",
    "    CLASS = 'class'\n",
    "\n",
    "    k = 5\n",
    "\n",
    "    features = np.array_split(original_features, k)\n",
    "    labels = np.array_split(original_labels, k)\n",
    "\n",
    "    accuracies = []\n",
    "    errors = []\n",
    "\n",
    "    print('=================================================')\n",
    "    print('Executando para ' + str(num_boosts) + ' Stumps')\n",
    "\n",
    "\n",
    "    \n",
    "    # Exec k fold\n",
    "    for i in range(k):\n",
    "        print(('Fold '+str(i+1)).ljust(10), end=\" \")\n",
    "\n",
    "        # Set up test data\n",
    "        test = {FEATURES: features[i], CLASS: labels[i].astype(int)}\n",
    "\n",
    "        # Remaining data as training\n",
    "        remaining_features = np.concatenate(np.delete(features, i))\n",
    "        remaining_labels = np.concatenate(np.delete(labels, i))\n",
    "\n",
    "        train = {FEATURES: remaining_features, CLASS: remaining_labels.astype(int)}\n",
    "\n",
    "        # Instance Classifier\n",
    "        boosting = AdaBoost(train, test)\n",
    "\n",
    "        # Exec classifier with X steps\n",
    "        accuracy,error,models = boosting.boost(num_boosts)\n",
    "\n",
    "        # Print accuracy and error for each fold\n",
    "        print('Accuracy: %.2f'%accuracy+'%', end=\"\\t\") \n",
    "        print('Error: %.2f'%error+'%')\n",
    "        accuracies.append(accuracy)\n",
    "        errors.append(error)\n",
    "\n",
    "    # Find mean accuracy and error and print on notebook\n",
    "    accuracy = np.divide(np.sum(accuracies), k)\n",
    "    error = np.divide(np.sum(errors), k)\n",
    "\n",
    "    print('-------------------------------------------------')\n",
    "    print('Final'.ljust(10), end=' ')\n",
    "    print('Accuracy: %.2f'%accuracy+'%', end='\\t')\n",
    "    print('Error: %.2f'%error+'%')\n",
    "    print('=================================================')\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fim de entender o impacto do número de boostings na implemetação, foi feita uma comparação de acurácia e erro na execussão dos valores de 10, 50, 100, 500 e 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================\n",
      "Executando para 10 Stumps\n",
      "Fold 1     Accuracy: 75.00%\tError: 25.00%\n",
      "Fold 2     Accuracy: 72.92%\tError: 27.08%\n",
      "Fold 3     Accuracy: 75.52%\tError: 24.48%\n",
      "Fold 4     Accuracy: 70.68%\tError: 29.32%\n",
      "Fold 5     Accuracy: 74.35%\tError: 25.65%\n",
      "-------------------------------------------------\n",
      "Final      Accuracy: 73.69%\tError: 26.31%\n",
      "=================================================\n",
      "=================================================\n",
      "Executando para 50 Stumps\n",
      "Fold 1     Accuracy: 82.81%\tError: 17.19%\n",
      "Fold 2     Accuracy: 79.17%\tError: 20.83%\n",
      "Fold 3     Accuracy: 84.90%\tError: 15.10%\n",
      "Fold 4     Accuracy: 79.06%\tError: 20.94%\n",
      "Fold 5     Accuracy: 85.86%\tError: 14.14%\n",
      "-------------------------------------------------\n",
      "Final      Accuracy: 82.36%\tError: 17.64%\n",
      "=================================================\n",
      "=================================================\n",
      "Executando para 100 Stumps\n",
      "Fold 1     Accuracy: 91.15%\tError: 8.85%\n",
      "Fold 2     Accuracy: 86.98%\tError: 13.02%\n",
      "Fold 3     Accuracy: 91.15%\tError: 8.85%\n",
      "Fold 4     Accuracy: 85.86%\tError: 14.14%\n",
      "Fold 5     Accuracy: 91.10%\tError: 8.90%\n",
      "-------------------------------------------------\n",
      "Final      Accuracy: 89.25%\tError: 10.75%\n",
      "=================================================\n",
      "=================================================\n",
      "Executando para 500 Stumps\n",
      "Fold 1     Accuracy: 98.44%\tError: 1.56%\n",
      "Fold 2     Accuracy: 97.40%\tError: 2.60%\n",
      "Fold 3     Accuracy: 98.96%\tError: 1.04%\n",
      "Fold 4     Accuracy: 97.38%\tError: 2.62%\n",
      "Fold 5     Accuracy: 98.95%\tError: 1.05%\n",
      "-------------------------------------------------\n",
      "Final      Accuracy: 98.23%\tError: 1.77%\n",
      "=================================================\n",
      "=================================================\n",
      "Executando para 1000 Stumps\n",
      "Fold 1     Accuracy: 98.96%\tError: 1.04%\n",
      "Fold 2     Accuracy: 97.40%\tError: 2.60%\n",
      "Fold 3     Accuracy: 98.96%\tError: 1.04%\n",
      "Fold 4     Accuracy: 97.38%\tError: 2.62%\n",
      "Fold 5     Accuracy: 98.95%\tError: 1.05%\n",
      "-------------------------------------------------\n",
      "Final      Accuracy: 98.33%\tError: 1.67%\n",
      "=================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'Número de Stumps')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_run = []\n",
    "\n",
    "num_boosts = [10, 50, 100, 500, 1000]\n",
    "\n",
    "for num in num_boosts:\n",
    "    plot_run.append(run(num))\n",
    "\n",
    "plt.title('Precisão para número de Stumps')\n",
    "plt.plot(num_boosts, plot_run, linestyle='-', marker='*', color='r')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.xlabel('Número de Stumps')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A diferenciação entre 500 e 1000 boosts se torna irrelevante. Isto mostra que o algoritmo atingiu a otimização antes de rodar com 1000 bosts. Como há uso de aleatoriedade na dispisição dos dados, é possível achar resultados que saiam um pouco do escopo esperado em execuções."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referência\n",
    "\n",
    "https://machinelearningmastery.com/boosting-and-adaboost-for-machine-learning/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
